{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto click on all regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 08:19:49,580 - INFO - ====== WebDriver manager ======\n",
      "2024-06-24 08:19:50,761 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2024-06-24 08:19:51,267 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2024-06-24 08:19:51,774 - INFO - Driver [C:\\Users\\ibrah\\.wdm\\drivers\\chromedriver\\win64\\126.0.6478.63\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2024-06-24 08:19:52,875 - INFO - Opening the webpage...\n",
      "2024-06-24 08:20:13,771 - INFO - Webpage opened successfully.\n",
      "2024-06-24 08:20:13,772 - INFO - Selecting 'All Regions'...\n",
      "2024-06-24 08:20:13,924 - INFO - Selected 'All Regions'. Waiting for listings to load...\n",
      "2024-06-24 08:28:23,968 - ERROR - Error extracting listing URLs: Message: \n",
      "\n",
      "2024-06-24 08:28:23,969 - INFO - Saved 0 URLs to escape_room_urls.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "def initialize_driver():\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the webpage\n",
    "def open_webpage(driver, url):\n",
    "    logging.info(\"Opening the webpage...\")\n",
    "    driver.get(url)\n",
    "    logging.info(\"Webpage opened successfully.\")\n",
    "\n",
    "# Select \"All Regions\" and wait for the listings to load\n",
    "def select_all_regions(driver):\n",
    "    try:\n",
    "        logging.info(\"Selecting 'All Regions'...\")\n",
    "        select_element = WebDriverWait(driver, 15).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"#search_region_chosen a.chosen-single\"))\n",
    "        )\n",
    "        select_element.click()\n",
    "        all_regions_option = WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"#search_region_chosen .chosen-results li[data-option-array-index='0']\"))\n",
    "        )\n",
    "        all_regions_option.click()\n",
    "        logging.info(\"Selected 'All Regions'. Waiting for listings to load...\")\n",
    "        time.sleep(400)  # Wait for 10 seconds to allow listings to load\n",
    "    except (TimeoutException, WebDriverException) as e:\n",
    "        logging.error(f\"Error selecting 'All Regions': {e}\")\n",
    "\n",
    "# Extract URLs of the listings\n",
    "def extract_listing_urls(driver):\n",
    "    try:\n",
    "        listing_elements = WebDriverWait(driver, 90).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".job_listing .job_listing-clickbox\"))\n",
    "        )\n",
    "        urls = [element.get_attribute(\"href\") for element in listing_elements]\n",
    "        logging.info(f\"Found {len(urls)} listings.\")\n",
    "        return urls\n",
    "    except (TimeoutException, WebDriverException, NoSuchElementException) as e:\n",
    "        logging.error(f\"Error extracting listing URLs: {e}\")\n",
    "        return []\n",
    "\n",
    "# Save URLs to a JSON file\n",
    "def save_urls(urls, filename='escape_room_urls.json'):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(urls, json_file, indent=4)\n",
    "    logging.info(f\"Saved {len(urls)} URLs to {filename}\")\n",
    "\n",
    "def main():\n",
    "    driver = initialize_driver()\n",
    "    try:\n",
    "        open_webpage(driver, \"https://www.escaperoomdirectory.co.uk/listing-region/serbia/\")\n",
    "        select_all_regions(driver)\n",
    "        urls = extract_listing_urls(driver)\n",
    "        save_urls(urls)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually click on the regions and control the wait time before scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the webpage...\n",
      "Webpage opened successfully.\n",
      "Waiting for 120 seconds to allow the page to load completely...\n",
      "Successfully extracted 32 links and saved to '20.links.txt'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the webpage\n",
    "print(\"Opening the webpage...\")\n",
    "driver.get(\"https://www.escaperoomdirectory.co.uk/listing-region/serbia/\")\n",
    "print(\"Webpage opened successfully.\")\n",
    "\n",
    "# Wait for 30 seconds to allow the page to load completely\n",
    "print(\"Waiting for 120 seconds to allow the page to load completely...\")\n",
    "time.sleep(120)\n",
    "\n",
    "try:\n",
    "    # Function to save the list of links to a file\n",
    "    def save_links(links):\n",
    "        with open('20.links.txt', 'w') as file:\n",
    "            for link in links:\n",
    "                file.write(f\"{link}\\n\")\n",
    "\n",
    "    # Function to get the list of links from the listings\n",
    "    def get_links():\n",
    "        links = []\n",
    "        listings = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//ul[@class='job_listings listing-cards-anchor--active']/li\"))\n",
    "        )\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                link = listing.find_element(By.XPATH, \".//a[contains(@class, 'job_listing-clickbox')]\").get_attribute(\"href\")\n",
    "                links.append(link)\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "        return links\n",
    "\n",
    "    # Get the list of links\n",
    "    links = get_links()\n",
    "\n",
    "    # Save the links to a file\n",
    "    save_links(links)\n",
    "\n",
    "    # Print success message with the count of links\n",
    "    print(f\"Successfully extracted {len(links)} links and saved to '20.links.txt'.\")\n",
    "\n",
    "except TimeoutException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except WebDriverException as e:\n",
    "    print(f\"WebDriver error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the list of (url (for more details), name, address, phone) # WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the webpage...\n",
      "Webpage opened successfully.\n",
      "Waiting for 120 seconds to allow the page to load completely...\n",
      "Successfully extracted 1 entries.\n",
      "Successfully extracted data and saved to '20.Data.json'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the webpage\n",
    "print(\"Opening the webpage...\")\n",
    "driver.get(\"https://www.escaperoomdirectory.co.uk/listing-region/serbia/\")\n",
    "print(\"Webpage opened successfully.\")\n",
    "\n",
    "# Wait for 120 seconds to allow the page to load completely\n",
    "print(\"Waiting for 120 seconds to allow the page to load completely...\")\n",
    "time.sleep(140)\n",
    "\n",
    "try:\n",
    "    # Function to get the list of links, names, addresses, and phone numbers from the listings\n",
    "    def get_links_names_addresses_phones():\n",
    "        data = []\n",
    "        listings = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//ul[@class='job_listings listing-cards-anchor--active']/li\"))\n",
    "        )\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                link = listing.find_element(By.XPATH, \".//a[contains(@class, 'job_listing-clickbox')]\").get_attribute(\"href\")\n",
    "            except NoSuchElementException:\n",
    "                link = None\n",
    "            \n",
    "            try:\n",
    "                name = listing.find_element(By.XPATH, \".//h3[contains(@class, 'job_listing-title')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                name = None\n",
    "            \n",
    "            try:\n",
    "                address = listing.find_element(By.XPATH, \".//div[contains(@class, 'job_listing-location')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                address = None\n",
    "            \n",
    "            try:\n",
    "                phone_number = listing.find_element(By.XPATH, \".//div[contains(@class, 'job_listing-phone')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                phone_number = None\n",
    "            \n",
    "            data.append({\"url\": link, \"name\": name, \"address\": address, \"phone\": phone_number})\n",
    "        \n",
    "        return data\n",
    "\n",
    "    # Get the list of links, names, addresses, and phone numbers\n",
    "    data = get_links_names_addresses_phones()\n",
    "    print(f\"Successfully extracted {len(data)} entries.\")\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open('20.Data.json', 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    # Print success message\n",
    "    print(f\"Successfully extracted data and saved to '20.Data.json'.\")\n",
    "\n",
    "except TimeoutException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except WebDriverException as e:\n",
    "    print(f\"WebDriver error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Navigation (auto navigation does not show the full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the webpage...\n",
      "Webpage opened successfully.\n",
      "Waiting for 120 seconds to allow the page to load completely...\n",
      "Scraping page 1...\n",
      "Successfully extracted 32 entries from page 1.\n",
      "Data up to page 1 saved to 'escaperooms_serbia.json'.\n",
      "Waiting for 100 seconds for the next page to load...\n",
      "Scraping page 2...\n",
      "Successfully extracted 32 entries from page 2.\n",
      "Data up to page 2 saved to 'escaperooms_serbia.json'.\n",
      "Waiting for 100 seconds for the next page to load...\n",
      "Scraping page 3...\n",
      "Successfully extracted 32 entries from page 3.\n",
      "Data up to page 3 saved to 'escaperooms_serbia.json'.\n",
      "Waiting for 100 seconds for the next page to load...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Scrape data from all pages\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mscrape_all_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 78\u001b[0m, in \u001b[0;36mscrape_all_pages\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Wait for the next page to load\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for 100 seconds for the next page to load...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# No more pages to scrape\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more pages to scrape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the webpage\n",
    "print(\"Opening the webpage...\")\n",
    "driver.get(\"https://www.escaperoomdirectory.co.uk/listing-region/serbia/\")\n",
    "print(\"Webpage opened successfully.\")\n",
    "\n",
    "# Wait for 120 seconds to allow the page to load completely\n",
    "print(\"Waiting for 120 seconds to allow the page to load completely...\")\n",
    "time.sleep(120)\n",
    "\n",
    "try:\n",
    "    all_data = []\n",
    "\n",
    "    # Function to get the list of links, names, addresses, and phone numbers from the listings\n",
    "    def get_links_names_addresses_phones():\n",
    "        data = []\n",
    "        listings = WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//ul[@class='job_listings listing-cards-anchor--active']/li\"))\n",
    "        )\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                link = listing.find_element(By.XPATH, \".//a[contains(@class, 'job_listing-clickbox')]\").get_attribute(\"href\")\n",
    "            except NoSuchElementException:\n",
    "                link = None\n",
    "            \n",
    "            try:\n",
    "                name = listing.find_element(By.XPATH, \".//h3[contains(@class, 'job_listing-title')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                name = None\n",
    "            \n",
    "            try:\n",
    "                address = listing.find_element(By.XPATH, \".//div[contains(@class, 'job_listing-location')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                address = None\n",
    "            \n",
    "            try:\n",
    "                phone_number = listing.find_element(By.XPATH, \".//div[contains(@class, 'job_listing-phone')]\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                phone_number = None\n",
    "            \n",
    "            data.append({\"url\": link, \"name\": name, \"address\": address, \"phone\": phone_number})\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def scrape_all_pages():\n",
    "        current_page = 1\n",
    "        while True:\n",
    "            print(f\"Scraping page {current_page}...\")\n",
    "            # Scrape data from the current page\n",
    "            page_data = get_links_names_addresses_phones()\n",
    "            all_data.extend(page_data)\n",
    "            print(f\"Successfully extracted {len(page_data)} entries from page {current_page}.\")\n",
    "\n",
    "            # Save the data to a JSON file after scraping each page\n",
    "            with open('escaperooms_serbia.json', 'w') as file:\n",
    "                json.dump(all_data, file, indent=4)\n",
    "            print(f\"Data up to page {current_page} saved to 'escaperooms_serbia.json'.\")\n",
    "\n",
    "            try:\n",
    "                # Find the next page button\n",
    "                next_page_button = driver.find_element(By.XPATH, f\"//a[@data-page='{current_page + 1}']\")\n",
    "                next_page_button.click()\n",
    "                current_page += 1\n",
    "                # Wait for the next page to load\n",
    "                print(\"Waiting for 100 seconds for the next page to load...\")\n",
    "                time.sleep(100)\n",
    "            except NoSuchElementException:\n",
    "                # No more pages to scrape\n",
    "                print(\"No more pages to scrape.\")\n",
    "                break\n",
    "\n",
    "    # Scrape data from all pages\n",
    "    scrape_all_pages()\n",
    "\n",
    "except TimeoutException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except WebDriverException as e:\n",
    "    print(f\"WebDriver error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
